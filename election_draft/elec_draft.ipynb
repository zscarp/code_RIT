{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67f743f",
   "metadata": {},
   "source": [
    "Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5193cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv \n",
    "from torch_geometric.utils import degree\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from torch_geometric.utils import dropout_adj\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def collect_candidate_data():\n",
    "    election_years = range(1988, 2025, 4)\n",
    "    candidates = {}\n",
    "    base_url = \"https://littlesis.org/api\"\n",
    "    print(f\"Collecting candidates from {base_url} :\")\n",
    "    for year in election_years:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{year}_United_States_presidential_election\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        infobox = soup.find('table', class_='infobox')\n",
    "        date = \"Unknown\"\n",
    "        if infobox:\n",
    "            rows = infobox.find_all('tr')\n",
    "            for row in rows:\n",
    "                th = row.find('th')\n",
    "                if th and ('←' in th.get_text() or '→' in th.get_text()):\n",
    "                    date_text = th.get_text(strip=True)\n",
    "                    date_match = re.search(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b', date_text)\n",
    "                    if date_match:\n",
    "                        date = parser.parse(date_match.group(0)).strftime(\"%Y-%m-%d\")\n",
    "                        break\n",
    "            if date == \"Unknown\":\n",
    "                for row in rows:\n",
    "                    for tag in row.find_all(['th', 'td']):\n",
    "                        date_match = re.search(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b', tag.get_text(strip=True))\n",
    "                        if date_match:\n",
    "                            date = parser.parse(date_match.group(0)).strftime(\"%Y-%m-%d\")\n",
    "                            break\n",
    "                    if date != \"Unknown\":\n",
    "                        break\n",
    "            \n",
    "            nominee_row = party_row = vote_row = None\n",
    "            for row in rows:\n",
    "                th = row.find('th')\n",
    "                if th:\n",
    "                    header_text = th.text.strip().lower()\n",
    "                    if \"nominee\" in header_text:\n",
    "                        nominee_row = row\n",
    "                    elif \"party\" in header_text:\n",
    "                        party_row = row\n",
    "                    elif \"electoral\" in header_text:\n",
    "                        vote_row = row\n",
    "            \n",
    "            if nominee_row and party_row and vote_row:\n",
    "                names = [td.text.strip() for td in nominee_row.find_all('td')]\n",
    "                parties = [td.text.strip() for td in party_row.find_all('td')]\n",
    "                votes = [td.text.strip() for td in vote_row.find_all('td')]\n",
    "                \n",
    "                for name, party, vote in zip(names, parties, votes):\n",
    "                    match = re.match(r'^\\d+', str(vote))\n",
    "                    vote = int(match.group()) if match else 0\n",
    "                    lit = f\"{base_url}/entities/search\"\n",
    "                    try:\n",
    "                        uid = requests.get(lit, params={\"q\": name}).json()[\"data\"][0][\"id\"]\n",
    "                    except (IndexError, KeyError):\n",
    "                        uid = None\n",
    "                    defender = 0\n",
    "                    party_defender = 0\n",
    "                    if year-4 in range(1988, 2025) and party in (\"Republican\", \"Democratic\"):\n",
    "                        prev_cand = candidates.get((str(year-4), party), {})\n",
    "                        if uid == prev_cand.get(\"id\") and prev_cand.get(\"win\") == 1:\n",
    "                            defender = 1\n",
    "                        party_defender = prev_cand.get(\"win\")\n",
    "                    types = []\n",
    "                    if uid:\n",
    "                        try:\n",
    "                            data_resp = requests.get(f\"{base_url}/entities/{uid}\").json()\n",
    "                            types = data_resp[\"data\"][\"attributes\"].get(\"types\", [])\n",
    "                            time.sleep(1)\n",
    "                        except:\n",
    "                            print(f\"Could not fetch types for {name}\")\n",
    "\n",
    "                    if party in (\"Republican\", \"Democratic\"):\n",
    "                        candidates[(str(year), party)] = {\n",
    "                            \"id\": uid, \"name\": name, \"defender\": defender,\n",
    "                            \"party_defender\": party_defender, \"vote\": vote, \"date\": date, \n",
    "                            \"win\": 1 if vote > 269 else 0,\"types\": types\n",
    "                        }\n",
    "        time.sleep(1)\n",
    "    save_data(candidates,\"candidates.json\")\n",
    "    return candidates\n",
    "\n",
    "def append_rel(relationships, rel, category):\n",
    "    relationships.append({\n",
    "        \"source\": rel[\"entity1_id\"],\n",
    "        \"target\": rel[\"entity2_id\"],\n",
    "        \"source_name\": \"Unknown\",\n",
    "        \"types\": \"Unknown\",\n",
    "        \"category\": category,\n",
    "        \"amount\": rel[\"amount\"],\n",
    "        \"filings\": rel[\"filings\"],\n",
    "        \"start_date\": rel[\"start_date\"],\n",
    "        \"current\": rel[\"is_current\"],\n",
    "        \"rel_id\": rel[\"id\"]\n",
    "    })\n",
    "\n",
    "def add_type(relationships, candidates, typemap):\n",
    "    for candidate in candidates:\n",
    "        for rel in relationships[candidate]:\n",
    "            rel_id = int(rel[\"rel_id\"])\n",
    "            if rel_id in typemap:\n",
    "                types, source_name = typemap[rel_id]\n",
    "                rel[\"source_name\"] = source_name\n",
    "                rel[\"types\"] = types\n",
    "\n",
    "def unique_types(relationships):\n",
    "    typelist = set()\n",
    "    for candidate in relationships.keys():\n",
    "        for rel in relationships[candidate]:\n",
    "            for t in rel[\"types\"]:\n",
    "                typelist.add(t)\n",
    "    return sorted(list(typelist))\n",
    "\n",
    "def collect_types(candidates, rela_ids):\n",
    "    mapped_types = {}\n",
    "    BASE_URL = \"https://littlesis.org/api\"\n",
    "    per_page = 300\n",
    "    candidate_ids = []\n",
    "    for candidate in candidates:\n",
    "        page = 1\n",
    "        count = 0\n",
    "        ids = candidates[candidate][\"id\"]\n",
    "        if ids in candidate_ids or ids is None:\n",
    "            continue\n",
    "        while True:\n",
    "            params = {\"page\": page, \"per_page\": per_page}\n",
    "            response = requests.get(f\"{BASE_URL}/entities/{ids}/connections\", params=params)\n",
    "            if response.status_code != 200 or not response.json():\n",
    "                break\n",
    "            data = response.json()\n",
    "            for f_data in data[\"data\"]:\n",
    "                rel_ids = f_data[\"attributes\"].get(\"connected_relationship_ids\", \"\").split(\",\")\n",
    "                types = f_data[\"attributes\"].get(\"types\", \"\")\n",
    "                name = f_data[\"attributes\"].get(\"name\", \"\")\n",
    "                for rel_id in rel_ids:\n",
    "                    rel_id = rel_id.strip()\n",
    "                    if rel_id and rel_id.isdigit() and int(rel_id) in rela_ids:\n",
    "                        mapped_types[int(rel_id)] = (types,name)\n",
    "                        #print(mapped_types[int(rel_id)][0],mapped_types[int(rel_id)][1])\n",
    "                        count += 1\n",
    "            page += 1\n",
    "            time.sleep(1)\n",
    "            if len(data[\"data\"]) < per_page:\n",
    "                candidate_ids.append(ids)\n",
    "                print(f'add {count} connection types of candidate {candidates[candidate][\"name\"]}')\n",
    "                break\n",
    "    return mapped_types\n",
    "\n",
    "def collect_relationships(candidates):\n",
    "    relationships = {}\n",
    "    BASE_URL = \"https://littlesis.org/api\"\n",
    "    print(f\"Collecting relationships from {BASE_URL} :\")\n",
    "    per_page = 300\n",
    "    endorsement_keywords = [\"endorser\", \"endorse\", \"support\", \"back\", \"campaign for\", \"advocate for\"]\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        page = 1\n",
    "        relationships[candidate] = []\n",
    "        if candidates[candidate][\"id\"] is None:\n",
    "            continue\n",
    "        ownes = 0\n",
    "        endoser = 0\n",
    "        while True:\n",
    "            count = 0\n",
    "            num = 0\n",
    "            endorsement = 0\n",
    "            ownership = 0\n",
    "            params = {\"page\": page, \"per_page\": per_page}\n",
    "            ids = candidates[candidate][\"id\"]\n",
    "            response = requests.get(f\"{BASE_URL}/entities/{ids}/relationships\", params=params)\n",
    "            if response.status_code != 200 or not response.json():\n",
    "                break\n",
    "            data = response.json()\n",
    "            for f_data in data[\"data\"]:\n",
    "                start_date_str = f_data[\"attributes\"].get(\"start_date\", \"\") or \"\"\n",
    "                #end_date_str = f_data[\"attributes\"].get(\"end_date\", \"\") or \"\"\n",
    "                if start_date_str :\n",
    "                    parts = start_date_str.split(\"-\")\n",
    "                    if len(parts) == 3:  # Ensure format is YYYY-MM-DD\n",
    "                        year, month, day = parts\n",
    "                        #yeare,monthe,daye = parte\n",
    "                        #yeare = str(int(yeare)-4)\n",
    "                        month = \"01\" if month == \"00\" else month  # Replace invalid month\n",
    "                        day = \"01\" if day == \"00\" else day  # Replace invalid day\n",
    "                        start_date_str = \"-\".join([year, month, day])\n",
    "                        #monthe = \"01\" if monthe == \"00\" else month  # Replace invalid month\n",
    "                        #daye = \"01\" if daye == \"00\" else day  # Replace invalid day\n",
    "                        #end_date_str = \"-\".join([yeare, monthe, daye])\n",
    "                if not start_date_str:\n",
    "                    continue\n",
    "                try:\n",
    "                    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "                    #end_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                elec_date = datetime.strptime(candidates[candidate][\"date\"], \"%Y-%m-%d\").date()\n",
    "                attrs = f_data[\"attributes\"]\n",
    "    \n",
    "                if not (start_date > elec_date and start_date_str and int(candidate[0]) > 1992) :\n",
    "                        #or (end_date < elec_date and end_date_str and int(candidate[0]) > 1992))\n",
    "                    if attrs[\"category_id\"] == 5 and attrs[\"entity2_id\"] == ids:\n",
    "                        append_rel(relationships[candidate], attrs, \"donor\")\n",
    "                        count += 1\n",
    "                    elif attrs[\"category_id\"] == 10:\n",
    "                        append_rel(relationships[candidate], attrs, \"ownership\")\n",
    "                        count += 1\n",
    "                        ownership += 1\n",
    "                    elif any(keyword in (attrs.get(\"description1\", \"\") or \"\").lower() for keyword in endorsement_keywords):\n",
    "                        append_rel(relationships[candidate], attrs, \"endorser\")\n",
    "                        count += 1\n",
    "                        endorsement += 1\n",
    "                    \n",
    "            p = len(data[\"data\"])\n",
    "            endoser += endorsement\n",
    "            ownes += ownership\n",
    "            if page%10 == 0:\n",
    "                print(f\"Fetched {page} pages \")\n",
    "                print(f\"{candidates[candidate]['name']} added total {len(relationships[candidate])} items, {endoser} endorser, {ownes} ownerships\")\n",
    "            if len(data[\"data\"]) < per_page:\n",
    "                print(f'candidates: {candidate}, {candidates[candidate][\"name\"]} collected {len(relationships[candidate])} relationships')\n",
    "                break\n",
    "            page += 1\n",
    "            time.sleep(1.0)\n",
    "    \n",
    "    relationship_ids = {rel[\"rel_id\"] for candidate in candidates for rel in relationships[candidate]}\n",
    "    typemap = collect_types(candidates, relationship_ids)\n",
    "    add_type(relationships, candidates, typemap)\n",
    "    save_data(relationships,\"relationships.json\")\n",
    "    return relationships\n",
    "\n",
    "def save_data(data, filename):\n",
    "    data_to_save = {f\"{k[0]}_{k[1]}\": v for k, v in data.items()} if isinstance(next(iter(data), None), tuple) else data\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_to_save, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def load_data(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        candidates = collect_candidate_data()\n",
    "        data = collect_relationships(candidates)\n",
    "    else:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    # Check if keys are like '1988_Democratic' and convert to tuple\n",
    "    first_key = next(iter(data), None)\n",
    "    if isinstance(first_key, str) and '_' in first_key:\n",
    "        return { (k.split('_')[0], k.split('_', 1)[1]) : v for k, v in data.items()}\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f734b",
   "metadata": {},
   "source": [
    "Build and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "233a795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_election_graphs(candidates, relationships):\n",
    "    \"\"\"Builds a dictionary of Data objects, one for each election year, using local types.\"\"\"\n",
    "    election_graphs = {}\n",
    "    all_entity_types_set = set()\n",
    "    entity_id_to_types_map = {}\n",
    "\n",
    "    print(\"Gathering all entity types from local data...\")\n",
    "    for cand_info in candidates.values():\n",
    "        if cand_info.get(\"id\"):\n",
    "            cand_types = {t['name'] for t in cand_info.get(\"types\", []) if isinstance(t, dict) and 'name' in t}\n",
    "            cand_types.add(\"Political Candidate\")\n",
    "            entity_id_to_types_map.setdefault(cand_info[\"id\"], set()).update(cand_types)\n",
    "            all_entity_types_set.update(cand_types)\n",
    "\n",
    "    for rel_list in relationships.values():\n",
    "        for rel in rel_list:\n",
    "            if rel.get(\"source\"):\n",
    "                src_types = rel.get(\"types\", [\"Unknown\"])\n",
    "                src_types = src_types if isinstance(src_types, list) else [src_types]\n",
    "                entity_id_to_types_map.setdefault(rel[\"source\"], set()).update(src_types)\n",
    "                all_entity_types_set.update(src_types)\n",
    "    \n",
    "    all_entity_types_set.add(\"Unknown\")\n",
    "    sorted_entity_types = sorted(list(filter(None, all_entity_types_set)))\n",
    "    type_to_idx = {t: i for i, t in enumerate(sorted_entity_types)}\n",
    "    num_entity_types = len(sorted_entity_types)\n",
    "    \n",
    "    # Hardcoded edge categories for consistency\n",
    "    cat_to_idx = {\"donor\": 0, \"endorser\": 1, \"ownership\": 2}\n",
    "    num_edge_cats = len(cat_to_idx)\n",
    "\n",
    "    # Node Features: [is_cand_A, is_cand_B, is_incumbent] + OHE(entity_types)\n",
    "    num_node_base_features = 3\n",
    "    node_feature_dim = num_node_base_features + num_entity_types\n",
    "    edge_feature_dim = num_edge_cats + 1\n",
    "\n",
    "    print(\"Building election-specific graphs...\")\n",
    "    election_years = sorted(list(set(int(k[0]) for k in candidates.keys())))\n",
    "\n",
    "    for year_int in election_years:\n",
    "        year = str(year_int)\n",
    "        dem_key, rep_key = (year, \"Democratic\"), (year, \"Republican\")\n",
    "        if dem_key not in candidates or rep_key not in candidates:\n",
    "            continue\n",
    "\n",
    "        cand_A_info, cand_B_info = candidates[dem_key], candidates[rep_key]\n",
    "        cand_A_id, cand_B_id = cand_A_info[\"id\"], cand_B_info[\"id\"]\n",
    "\n",
    "        nodes_in_year = {cand_A_id, cand_B_id}\n",
    "        edges_in_year = []\n",
    "        \n",
    "        election_date = datetime.strptime(cand_A_info[\"date\"], \"%Y-%m-%d\").date()\n",
    "        for rel in relationships.get(dem_key, []) + relationships.get(rep_key, []):\n",
    "            start_date_str = rel.get(\"start_date\")\n",
    "            if not start_date_str: continue\n",
    "            try:\n",
    "                start_date = datetime.strptime(start_date_str.split(\"T\")[0], \"%Y-%m-%d\").date()\n",
    "                if start_date <= election_date:\n",
    "                    if rel.get(\"source\"): nodes_in_year.add(rel.get(\"source\"))\n",
    "                    if rel.get(\"target\"): nodes_in_year.add(rel.get(\"target\"))\n",
    "                    edges_in_year.append(rel)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        \n",
    "        nodes_in_year = sorted(list(n for n in nodes_in_year if n is not None))\n",
    "        if not nodes_in_year: continue\n",
    "        node_map = {nid: i for i, nid in enumerate(nodes_in_year)}\n",
    "\n",
    "        x = []\n",
    "        node_idx_to_primary_type = {}\n",
    "        for nid in nodes_in_year:\n",
    "            features = [0.0] * node_feature_dim\n",
    "            features[0] = 1.0 if nid == cand_A_id else 0.0\n",
    "            features[1] = 1.0 if nid == cand_B_id else 0.0\n",
    "            features[2] = cand_A_info[\"defender\"] if nid == cand_A_id else (cand_B_info[\"defender\"] if nid == cand_B_id else 0.0)\n",
    "            features[3] = cand_A_info[\"party_defender\"] if nid == cand_A_id else (cand_B_info[\"party_defender\"] if nid == cand_B_id else 0.0)\n",
    "            node_types = entity_id_to_types_map.get(nid, {\"Unknown\"})\n",
    "            primary_type = next((t for t in node_types if t != \"Unknown\"), \"Unknown\")\n",
    "            node_idx_to_primary_type[node_map[nid]] = primary_type\n",
    "            for nt in node_types:\n",
    "                if nt in type_to_idx:\n",
    "                    features[num_node_base_features + type_to_idx[nt]] = 1.0\n",
    "            x.append(features)\n",
    "\n",
    "        edge_indices, edge_attrs = [], []\n",
    "        for rel in edges_in_year:\n",
    "            src, tgt = rel.get(\"source\"), rel.get(\"target\")\n",
    "            if src in node_map and tgt in node_map:\n",
    "                edge_indices.append([node_map[src], node_map[tgt]])\n",
    "                attrs = [0.0] * edge_feature_dim\n",
    "                if rel[\"category\"] in cat_to_idx:\n",
    "                    attrs[cat_to_idx[rel[\"category\"]]] = 1.0\n",
    "                attrs[-1] = float(rel.get(\"amount\") or 0) / 1000000.0\n",
    "                edge_attrs.append(attrs)\n",
    "                print(\"amount: \",attrs[-1])\n",
    "                print(attrs)\n",
    "\n",
    "        data = Data(\n",
    "            x=torch.tensor(x, dtype=torch.float),\n",
    "            edge_index=torch.tensor(edge_indices, dtype=torch.long).t().contiguous() if edge_indices else torch.empty((2, 0), dtype=torch.long),\n",
    "            edge_attr=torch.tensor(edge_attrs, dtype=torch.float) if edge_attrs else None,\n",
    "            y_win=torch.tensor([cand_A_info[\"win\"], cand_B_info[\"win\"]], dtype=torch.long),\n",
    "            y_vote=torch.tensor([cand_A_info[\"vote\"] / 538.0, cand_B_info[\"vote\"] / 538.0], dtype=torch.float),\n",
    "            cand_A_idx=node_map.get(cand_A_id, -1), cand_B_idx=node_map.get(cand_B_id, -1),\n",
    "            year=year_int, cand_A_name=cand_A_info['name'], cand_B_name=cand_B_info['name'],\n",
    "            node_idx_to_primary_type=node_idx_to_primary_type\n",
    "        )\n",
    "        election_graphs[year_int] = data\n",
    "        \n",
    "    print(f\"Built {len(election_graphs)} graphs.\")\n",
    "    return election_graphs\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=4, dropout=0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout, add_self_loops=False)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=dropout, add_self_loops=False)\n",
    "        self.mlp = torch.nn.Linear(hidden_channels * 2, 4)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x, a1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x, a2 = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        emb_A = x[data.cand_A_idx]\n",
    "        emb_B = x[data.cand_B_idx]\n",
    "        combined = torch.cat([emb_A, emb_B], dim=0)\n",
    "        out = self.mlp(combined)\n",
    "        win_logits = out[:2].unsqueeze(0)\n",
    "        vote_pred = torch.sigmoid(out[2:])\n",
    "        return win_logits, vote_pred, (a1, a2)\n",
    "\n",
    "def train_loop(model, train_graphs, optimizer, vote_weight=10.0, adversarial_strength=0.2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    adversary = AdversarialTraining(attack_strength=adversarial_strength)\n",
    "    \n",
    "    for year in train_graphs:\n",
    "        data = train_graphs[year]\n",
    "        \n",
    "        # Adversarial training: alternate between clean and attacked data\n",
    "        if random.random() > 0.5:\n",
    "            data = adversary.apply_attack(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        win_logits, vote_pred, _ = model(data)\n",
    "        win_target = torch.argmax(data.y_win).unsqueeze(0)\n",
    "        loss_win = F.cross_entropy(win_logits, win_target)\n",
    "        loss_vote = F.mse_loss(vote_pred, data.y_vote)\n",
    "        loss = loss_win + vote_weight * loss_vote\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_graphs)\n",
    "\n",
    "def test_loop(model, test_graphs):\n",
    "    model.eval()\n",
    "    correct_wins, vote_acc, total_vote_mae, total_graphs = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for year in test_graphs:\n",
    "            data = test_graphs[year]\n",
    "            win_logits, vote_pred, _ = model(data)\n",
    "            if torch.argmax(win_logits.squeeze()) == torch.argmax(data.y_win):\n",
    "                correct_wins += 1\n",
    "            mask_vote = data.y_vote[:] != -1.0\n",
    "            pred_vote = vote_pred[mask_vote].squeeze()\n",
    "            true_vote = data.y_vote[mask_vote].squeeze()\n",
    "            if true_vote.numel() > 0:  # Check if there are valid vote targets\n",
    "                vote_acc += (pred_vote / true_vote).mean().item()\n",
    "            total_vote_mae += F.l1_loss(vote_pred * 538, data.y_vote * 538).item()\n",
    "            total_graphs += 1\n",
    "            \n",
    "    win_acc = correct_wins / total_graphs if total_graphs > 0 else 0\n",
    "    vote_acc = vote_acc / total_graphs if total_graphs > 0 else 0\n",
    "    if vote_acc > 1 : vote_acc = 1/vote_acc\n",
    "    vote_mae = total_vote_mae / total_graphs if total_graphs > 0 else 0\n",
    "    return win_acc, vote_acc, vote_mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63e247",
   "metadata": {},
   "source": [
    "Analyze and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f6a5e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, _, (_, attention_weights) = model(data)\n",
    "    edge_index, attn_w = attention_weights\n",
    "    all_types = set(data.node_idx_to_primary_type.values())\n",
    "    print(\"\\n## GAT Attention Analysis (Importance of Source Entity Types):\")\n",
    "    for cand_idx, cand_name in [(data.cand_A_idx.item(), data.cand_A_name), (data.cand_B_idx.item(), data.cand_B_name)]:\n",
    "        scores_per_type = {t: [] for t in all_types}\n",
    "        mask = (edge_index[1] == cand_idx)\n",
    "        for src_node_idx, weight in zip(edge_index[0][mask], attn_w[mask]):\n",
    "            src_type = data.node_idx_to_primary_type.get(src_node_idx.item(), \"Unknown\")\n",
    "            scores_per_type[src_type].append(weight.item())\n",
    "        avg_scores = {t: np.mean(s) if s else 0 for t, s in scores_per_type.items()}\n",
    "        print(f\"\\nAttention for {cand_name}:\")\n",
    "        sorted_scores = sorted(avg_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        for t, s in sorted_scores[:10]:\n",
    "            if s > 0: print(f\"  - {t}: {s:.4f}\")\n",
    "\n",
    "class AdversarialTraining:\n",
    "    def __init__(self, attack_types=['edge_drop', 'feature_noise'], attack_strength=0.2):\n",
    "        self.attack_types = attack_types\n",
    "        self.attack_strength = attack_strength\n",
    "    \n",
    "    def apply_attack(self, data):\n",
    "        \"\"\"Apply random attacks to graph data\"\"\"\n",
    "        attacked_data = deepcopy(data)\n",
    "        \n",
    "        if 'edge_drop' in self.attack_types and random.random() < 0.7:\n",
    "            edge_index, _ = dropout_adj(\n",
    "                data.edge_index, \n",
    "                p=self.attack_strength,\n",
    "                force_undirected=True\n",
    "            )\n",
    "            attacked_data.edge_index = edge_index\n",
    "        \n",
    "        if 'feature_noise' in self.attack_types:\n",
    "            noise = torch.randn_like(data.x) * self.attack_strength\n",
    "            attacked_data.x = data.x + noise\n",
    "            \n",
    "        return attacked_data\n",
    "\n",
    "class GeneticRewiringOptimizer:\n",
    "    def __init__(self, model, fitness_evaluator, population_size=10, generations=3, mutation_rate=0.1):\n",
    "        self.model = model\n",
    "        self.fitness_evaluator = fitness_evaluator\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "    \n",
    "    def optimize(self, original_data, candidate_idx):\n",
    "        \"\"\"Evolve network structure for better resilience\"\"\"\n",
    "        population = self._initialize_population(original_data)\n",
    "        \n",
    "        for gen in range(self.generations):\n",
    "            fitness_scores = []\n",
    "            for i, graph in enumerate(population):\n",
    "                fitness = self.fitness_evaluator.evaluate(self.model, graph, candidate_idx)\n",
    "                fitness_scores.append(fitness)\n",
    "            \n",
    "            # Select top performers\n",
    "            sorted_indices = np.argsort(fitness_scores)[::-1]\n",
    "            elite = [population[i] for i in sorted_indices[:2]]\n",
    "            \n",
    "            # Create next generation\n",
    "            new_population = elite[:]\n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1, parent2 = random.choices(elite, k=2)\n",
    "                child = self._crossover(parent1, parent2)\n",
    "                child = self._mutate(child)\n",
    "                new_population.append(child)\n",
    "            \n",
    "            population = new_population\n",
    "            print(f\"Generation {gen+1}: Max Fitness = {max(fitness_scores):.4f}\")\n",
    "        \n",
    "        return population[0]  # Return best graph\n",
    "\n",
    "    def _initialize_population(self, original_data):\n",
    "        \"\"\"Create initial population with variations\"\"\"\n",
    "        return [self._create_variant(original_data) for _ in range(self.population_size)]\n",
    "    \n",
    "    def _create_variant(self, data):\n",
    "        \"\"\"Create graph variant with random modifications\"\"\"\n",
    "        variant = deepcopy(data)\n",
    "        edge_index, _ = dropout_adj(\n",
    "            data.edge_index,\n",
    "            p=self.mutation_rate,\n",
    "            force_undirected=True\n",
    "        )\n",
    "        variant.edge_index = edge_index\n",
    "        return variant\n",
    "    \n",
    "    def _crossover(self, parent1, parent2):\n",
    "        \"\"\"Combine features of two parent graphs\"\"\"\n",
    "        child = deepcopy(parent1)\n",
    "        # Combine edge sets\n",
    "        combined_edges = torch.cat([parent1.edge_index, parent2.edge_index], dim=1)\n",
    "        unique_edges = torch.unique(combined_edges, dim=1)\n",
    "        child.edge_index = unique_edges\n",
    "        return child\n",
    "    \n",
    "    def _mutate(self, graph):\n",
    "        \"\"\"Apply random mutations to graph\"\"\"\n",
    "        # Random edge additions/removals\n",
    "        if random.random() < self.mutation_rate:\n",
    "            edge_index, _ = dropout_adj(\n",
    "                graph.edge_index,\n",
    "                p=0.1,\n",
    "                force_undirected=True\n",
    "            )\n",
    "            graph.edge_index = edge_index\n",
    "        return graph\n",
    "\n",
    "class ResilienceEvaluator:\n",
    "    def __init__(self, num_attacks=5, attack_strength=0.3):\n",
    "        self.num_attacks = num_attacks\n",
    "        self.attack_strength = attack_strength\n",
    "        self.adversarial = AdversarialTraining(attack_strength=attack_strength)\n",
    "    \n",
    "    def evaluate(self, model, data, candidate_idx):\n",
    "        \"\"\"Evaluate model resilience on attacked graphs\"\"\"\n",
    "        original_out = self._predict(model, data, candidate_idx)\n",
    "        stability_scores = []\n",
    "        \n",
    "        for _ in range(self.num_attacks):\n",
    "            attacked_data = self.adversarial.apply_attack(data)\n",
    "            attacked_out = self._predict(model, attacked_data, candidate_idx)\n",
    "            stability = 1.0 - torch.abs(original_out - attacked_out).mean().item()\n",
    "            stability_scores.append(stability)\n",
    "        \n",
    "        return np.mean(stability_scores)\n",
    "    \n",
    "    def _predict(self, model, data, candidate_idx):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            win_logits, vote_pred, _ = model(data)\n",
    "            # Map candidate_idx to 0 or 1 based on which candidate it represents\n",
    "            if candidate_idx == data.cand_A_idx:\n",
    "                cand_position = 0  # Corresponds to candidate A (e.g., Democratic)\n",
    "            elif candidate_idx == data.cand_B_idx:\n",
    "                cand_position = 1  # Corresponds to candidate B (e.g., Republican)\n",
    "            else:\n",
    "                raise ValueError(f\"candidate_idx {candidate_idx} does not match either candidate in the graph.\")\n",
    "            # Return win probability for the candidate\n",
    "        return F.softmax(win_logits, dim=1)[0, cand_position]\n",
    "\n",
    "def create_missing_edge_graphs(graphs, removal_rates):\n",
    "    \"\"\"\n",
    "    Creates corrupted graphs with missing edges at specified removal rates\n",
    "    Returns a dictionary: {removal_rate: {year: corrupted_graph}}\n",
    "    \"\"\"\n",
    "    corrupted_graphs = {}\n",
    "    for rate in removal_rates:\n",
    "        rate_graphs = {}\n",
    "        for year, data in graphs.items():\n",
    "            # Create a copy of the original graph\n",
    "            corrupted = deepcopy(data)\n",
    "            \n",
    "            # Randomly remove edges\n",
    "            edge_index, edge_attr = dropout_adj(\n",
    "                data.edge_index,\n",
    "                data.edge_attr,\n",
    "                p=rate,\n",
    "                force_undirected=True\n",
    "            )\n",
    "            \n",
    "            # Update with corrupted edges\n",
    "            corrupted.edge_index = edge_index\n",
    "            if edge_attr is not None:\n",
    "                corrupted.edge_attr = edge_attr\n",
    "            \n",
    "            rate_graphs[year] = corrupted\n",
    "        corrupted_graphs[rate] = rate_graphs\n",
    "    return corrupted_graphs\n",
    "\n",
    "def evaluate_corruption_robustness(model, corrupted_graphs, removal_rates):\n",
    "    \"\"\"\n",
    "    Evaluates model performance at different edge removal rates\n",
    "    Returns: (win_accuracies, vote_accuracies)\n",
    "    \"\"\"\n",
    "    win_accs = []\n",
    "    vote_accs = []\n",
    "    vote_maes = []\n",
    "    \n",
    "    for rate in removal_rates:\n",
    "        test_set = corrupted_graphs[rate]\n",
    "        win_acc, vote_acc, vote_mae = test_loop(model, test_set)\n",
    "        win_accs.append(win_acc)\n",
    "        vote_accs.append(vote_acc)\n",
    "        vote_maes.append(vote_mae)\n",
    "        print(f\"Removal Rate: {rate:.0%} | \"\n",
    "              f\"Win Acc: {win_acc:.4f} | \"\n",
    "              f\"Vote Acc: {vote_acc:.4f} | \"\n",
    "              f\"Vote MAE: {vote_mae:.2f}\")\n",
    "    \n",
    "    return win_accs, vote_accs, vote_maes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716a683",
   "metadata": {},
   "source": [
    "Pattern extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ebbebcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_influence(model, data, relationship_categories):\n",
    "    \"\"\"\n",
    "    Analyzes influence of different entity types using:\n",
    "    1. Attention weights\n",
    "    2. SHAP values\n",
    "    3. F1-score drop\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Attention-based importance\n",
    "    with torch.no_grad():\n",
    "        _, _, (_, attn_weights) = model(data)\n",
    "    edge_index, attn_values = attn_weights\n",
    "    \n",
    "    # Group attention by relationship type and source entity type\n",
    "    attn_importance = {cat: {} for cat in relationship_categories}\n",
    "    for i, edge in enumerate(edge_index.t()):\n",
    "        src_idx, tgt_idx = edge.tolist()\n",
    "        rel_type_idx = data.edge_attr[i][:3].argmax().item()\n",
    "        rel_category = list(relationship_categories.keys())[rel_type_idx]\n",
    "        src_type = data.node_idx_to_primary_type.get(src_idx, \"Unknown\")\n",
    "        \n",
    "        if src_type not in attn_importance[rel_category]:\n",
    "            attn_importance[rel_category][src_type] = []\n",
    "        attn_importance[rel_category][src_type].append(attn_values[i].item())\n",
    "    \n",
    "    # Aggregate attention scores\n",
    "    for cat, types in attn_importance.items():\n",
    "        for entity_type, scores in types.items():\n",
    "            attn_importance[cat][entity_type] = np.mean(scores)\n",
    "    \n",
    "    shap_importance = calculate_shap_values(model, data)\n",
    "\n",
    "    f1_drop_importance = calculate_f1_drop(model, data)\n",
    "    \n",
    "    # Combine metrics\n",
    "    for cat in relationship_categories:\n",
    "        for entity_type in set(attn_importance[cat].keys()) | set(shap_importance.keys()) | set(f1_drop_importance.keys()):\n",
    "            if entity_type not in results:\n",
    "                results[entity_type] = {\n",
    "                    'attention': 0,\n",
    "                    'shap': 0,\n",
    "                    'f1_drop': 0,\n",
    "                    'combined': 0\n",
    "                }\n",
    "            \n",
    "            # Weighted combination of metrics\n",
    "            results[entity_type]['attention'] += attn_importance[cat].get(entity_type, 0)\n",
    "            results[entity_type]['shap'] += shap_importance.get(entity_type, 0)\n",
    "            results[entity_type]['f1_drop'] += f1_drop_importance.get(entity_type, 0)\n",
    "    \n",
    "    # Normalize and combine scores\n",
    "    for entity_type in results:\n",
    "        # Normalize each metric to [0,1]\n",
    "        attn_norm = results[entity_type]['attention'] / max(1e-6, max(v['attention'] for v in results.values()))\n",
    "        shap_norm = results[entity_type]['shap'] / max(1e-6, max(v['shap'] for v in results.values()))\n",
    "        f1_drop_norm = results[entity_type]['f1_drop'] / max(1e-6, max(v['f1_drop'] for v in results.values()))\n",
    "        \n",
    "        # Weighted combination (adjust weights as needed)\n",
    "        results[entity_type]['combined'] = (\n",
    "            0.4 * attn_norm + \n",
    "            0.3 * shap_norm + \n",
    "            0.3 * f1_drop_norm\n",
    "        )\n",
    "    \n",
    "    # Get top 5 influential types\n",
    "    top_influential = sorted(\n",
    "        [(entity_type, scores['combined']) for entity_type, scores in results.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    return top_influential, results\n",
    "\n",
    "def calculate_shap_values(model, data, n_samples=50):\n",
    "    \"\"\"Approximates SHAP values using perturbation-based approach\"\"\"\n",
    "    baseline = torch.zeros_like(data.x)\n",
    "    shap_values = {}\n",
    "    \n",
    "    # Get original prediction\n",
    "    with torch.no_grad():\n",
    "        original_out, _, _ = model(data)\n",
    "        original_prob = F.softmax(original_out, dim=1)[0, 0].item()\n",
    "    \n",
    "    # Perturb each node type\n",
    "    for entity_type in set(data.node_idx_to_primary_type.values()):\n",
    "        total_effect = 0\n",
    "        node_indices = [\n",
    "            idx for idx, t in data.node_idx_to_primary_type.items() \n",
    "            if t == entity_type\n",
    "        ]\n",
    "        \n",
    "        if not node_indices:\n",
    "            continue\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Create perturbed graph\n",
    "            perturbed_data = deepcopy(data)\n",
    "            mask = torch.zeros(data.x.size(0), dtype=torch.bool)\n",
    "            mask[node_indices] = True\n",
    "            perturbed_data.x[mask] = baseline[mask]\n",
    "            \n",
    "            # Get perturbed prediction\n",
    "            with torch.no_grad():\n",
    "                perturbed_out, _, _ = model(perturbed_data)\n",
    "                perturbed_prob = F.softmax(perturbed_out, dim=1)[0, 0].item()\n",
    "            \n",
    "            total_effect += abs(original_prob - perturbed_prob)\n",
    "        \n",
    "        shap_values[entity_type] = total_effect / n_samples\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "def calculate_f1_drop(model, data):\n",
    "    \"\"\"Calculates F1-score drop when removing entity types, preserving candidates\"\"\"\n",
    "    # First get original F1 score\n",
    "    with torch.no_grad():\n",
    "        win_logits, _, _ = model(data)\n",
    "        pred = torch.argmax(win_logits.squeeze())\n",
    "        true = torch.argmax(data.y_win)\n",
    "        original_f1 = 1.0 if pred == true else 0.0\n",
    "    \n",
    "    f1_drop = {}\n",
    "    candidate_indices = {data.cand_A_idx, data.cand_B_idx}\n",
    "    \n",
    "    # Get all entity types, skipping empty strings\n",
    "    entity_types = set(t for t in data.node_idx_to_primary_type.values() if t.strip())\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        # Create graph without this entity type (preserving candidates)\n",
    "        modified_data = deepcopy(data)\n",
    "        node_mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        # Create list of nodes to remove\n",
    "        nodes_to_remove = []\n",
    "        for idx, t in data.node_idx_to_primary_type.items():\n",
    "            if t == entity_type and idx not in candidate_indices:\n",
    "                nodes_to_remove.append(idx)\n",
    "        \n",
    "        # Skip if no nodes to remove\n",
    "        if not nodes_to_remove:\n",
    "            f1_drop[entity_type] = 0.0\n",
    "            continue\n",
    "            \n",
    "        # Create node mask\n",
    "        for idx in nodes_to_remove:\n",
    "            node_mask[idx] = False\n",
    "        \n",
    "        # Apply node mask\n",
    "        modified_data.x = modified_data.x[node_mask]\n",
    "        \n",
    "        # Update edge index\n",
    "        row, col = modified_data.edge_index\n",
    "        mask = node_mask[row] & node_mask[col]\n",
    "        modified_data.edge_index = modified_data.edge_index[:, mask]\n",
    "        # Remap edge indices to new node indices\n",
    "        new_indices = torch.zeros(data.num_nodes, dtype=torch.long) - 1\n",
    "        new_idx = 0\n",
    "        for old_idx in range(data.num_nodes):\n",
    "            if node_mask[old_idx]:\n",
    "                new_indices[old_idx] = new_idx\n",
    "                new_idx += 1\n",
    "        modified_data.edge_index = new_indices[modified_data.edge_index]\n",
    "        \n",
    "        if modified_data.edge_attr is not None:\n",
    "            modified_data.edge_attr = modified_data.edge_attr[mask]\n",
    "        \n",
    "        # Update candidate indices\n",
    "        modified_data.cand_A_idx = new_indices[data.cand_A_idx].item()\n",
    "        modified_data.cand_B_idx = new_indices[data.cand_B_idx].item()\n",
    "        \n",
    "        # Update node_idx_to_primary_type\n",
    "        modified_data.node_idx_to_primary_type = {\n",
    "            new_indices[old_idx].item(): t\n",
    "            for old_idx, t in data.node_idx_to_primary_type.items()\n",
    "            if node_mask[old_idx]\n",
    "        }\n",
    "        \n",
    "        # Skip if candidates not found\n",
    "        if modified_data.cand_A_idx == -1 or modified_data.cand_B_idx == -1:\n",
    "            f1_drop[entity_type] = 0.0\n",
    "            continue\n",
    "            \n",
    "        # Get modified F1 score\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                win_logits, _, _ = model(modified_data)\n",
    "                pred = torch.argmax(win_logits.squeeze())\n",
    "                modified_f1 = 1.0 if pred == true else 0.0\n",
    "                f1_drop[entity_type] = original_f1 - modified_f1\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating F1 drop for {entity_type}: {str(e)}\")\n",
    "                f1_drop[entity_type] = 0.0\n",
    "    \n",
    "    return f1_drop\n",
    "\n",
    "def analyze_influence_across_years(model, graphs, relationship_categories):\n",
    "    \"\"\"Aggregates influence metrics across all election years\"\"\"\n",
    "    from collections import defaultdict\n",
    "    type_attn = defaultdict(list)\n",
    "    type_shap = defaultdict(list)\n",
    "    type_f1_drop = defaultdict(list)\n",
    "    \n",
    "    print(\"\\nAnalyzing influence across all election years...\")\n",
    "    \n",
    "    for year, data in graphs.items():\n",
    "        if year >= 2024:  # Skip prediction years\n",
    "            continue\n",
    "            \n",
    "        print(f\"  - Processing {year} election...\")\n",
    "        _, full_results = analyze_influence(model, data, relationship_categories)\n",
    "        \n",
    "        for entity_type, metrics in full_results.items():\n",
    "            # Skip empty entity types\n",
    "            if not entity_type.strip():\n",
    "                continue\n",
    "                \n",
    "            type_attn[entity_type].append(metrics['attention'])\n",
    "            type_shap[entity_type].append(metrics['shap'])\n",
    "            type_f1_drop[entity_type].append(metrics['f1_drop'])\n",
    "    \n",
    "    # Calculate average metrics per entity type\n",
    "    avg_metrics = {}\n",
    "    all_types = set(type_attn.keys()) | set(type_shap.keys()) | set(type_f1_drop.keys())\n",
    "    \n",
    "    for entity_type in all_types:\n",
    "        # Skip empty entity types\n",
    "        if not entity_type.strip():\n",
    "            continue\n",
    "            \n",
    "        attn_vals = type_attn.get(entity_type, [0])\n",
    "        shap_vals = type_shap.get(entity_type, [0])\n",
    "        f1_vals = type_f1_drop.get(entity_type, [0])\n",
    "        \n",
    "        # Compute means, skip if no valid values\n",
    "        attn_avg = np.mean(attn_vals) if attn_vals else 0\n",
    "        shap_avg = np.mean(shap_vals) if shap_vals else 0\n",
    "        f1_drop_avg = np.mean(f1_vals) if f1_vals else 0\n",
    "        \n",
    "        # Normalize metrics to comparable scales\n",
    "        attn_norm = min(attn_avg / 0.05, 1.0) if attn_avg > 0 else 0\n",
    "        shap_norm = min(shap_avg / 0.0005, 1.0) if shap_avg > 0 else 0\n",
    "        f1_drop_norm = min(f1_drop_avg / 0.2, 1.0) if f1_drop_avg > 0 else 0\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined = (0.4 * attn_norm + 0.3 * shap_norm + 0.3 * f1_drop_norm)\n",
    "        \n",
    "        avg_metrics[entity_type] = {\n",
    "            'attention': attn_avg,\n",
    "            'shap': shap_avg,\n",
    "            'f1_drop': f1_drop_avg,\n",
    "            'combined': combined\n",
    "        }\n",
    "    \n",
    "    # Get top 5 influential types\n",
    "    top_influential = sorted(\n",
    "        [(entity_type, metrics['combined']) for entity_type, metrics in avg_metrics.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    return top_influential, avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff5b9d",
   "metadata": {},
   "source": [
    "network plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cd7e6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_donor_network(year_to_plot='2020', min_donation=1000, label_threshold=50000):\n",
    "    \"\"\"\n",
    "    Enhanced visualization with:\n",
    "    - Increased node sizes\n",
    "    - Smaller font sizes to reduce label overlap\n",
    "    - Names for all donors and endorsers\n",
    "    - Node size based on donation amount\n",
    "    - Node color based on entity type\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    relationships = load_data('relationships.json')\n",
    "    candidates = load_data('candidates.json')\n",
    "\n",
    "    if not relationships or not candidates:\n",
    "        print(\"Could not load necessary data files.\")\n",
    "        return\n",
    "\n",
    "    # - Filter for relevant relationships ---\n",
    "    filtered_relations = []\n",
    "    candidate_info_this_year = {}\n",
    "    candidate_ids_this_year = set()\n",
    "\n",
    "    # Identify candidates\n",
    "    for key, info in candidates.items():\n",
    "        year, party = key\n",
    "        if year == year_to_plot and info.get('id'):\n",
    "            candidate_info_this_year[info['id']] = {\n",
    "                'name': info['name'], \n",
    "                'party': party\n",
    "            }\n",
    "            candidate_ids_this_year.add(info['id'])\n",
    "    \n",
    "    if not candidate_info_this_year:\n",
    "        print(f\"No candidate data found for {year_to_plot}.\")\n",
    "        return\n",
    "\n",
    "    # Collect relationships and aggregate donation amounts\n",
    "    donor_amounts = defaultdict(float)\n",
    "    for key, rel_list in relationships.items():\n",
    "        year, party = key\n",
    "        if year == year_to_plot:\n",
    "            for rel in rel_list:\n",
    "                category = rel.get('category')\n",
    "                source_id = rel.get('source')\n",
    "                \n",
    "                # For donors: track total donations per source\n",
    "                if category == 'donor':\n",
    "                    try:\n",
    "                        amount = float(rel.get('amount', 0))\n",
    "                        if amount > min_donation:\n",
    "                            filtered_relations.append(rel)\n",
    "                            donor_amounts[source_id] += amount\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                # For endorsers: just add\n",
    "                elif category == 'endorser':\n",
    "                    filtered_relations.append(rel)\n",
    "\n",
    "    if not filtered_relations:\n",
    "        print(f\"No relationships found for the year {year_to_plot} with the specified filters.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(filtered_relations)} filtered relationships for {year_to_plot}\")\n",
    "\n",
    "    # Build the Graph with Enhanced Attributes ---\n",
    "    G = nx.Graph()\n",
    "    node_attributes = {}\n",
    "    \n",
    "    # Add candidate nodes\n",
    "    for cand_id, info in candidate_info_this_year.items():\n",
    "        G.add_node(cand_id, \n",
    "                   type='candidate',\n",
    "                   party=info['party'],\n",
    "                   name=info['name'],\n",
    "                   label=info['name'])\n",
    "        node_attributes[cand_id] = {\n",
    "            'type': 'candidate',\n",
    "            'party': info['party'],\n",
    "            'name': info['name']\n",
    "        }\n",
    "\n",
    "    # Add donor/endorser nodes with attributes\n",
    "    for rel in filtered_relations:\n",
    "        source_id = rel.get('source')\n",
    "        target_id = rel.get('target')\n",
    "        category = rel.get('category')\n",
    "        \n",
    "        if not source_id or not target_id or target_id not in candidate_ids_this_year:\n",
    "            continue\n",
    "            \n",
    "        # Initialize node attributes if new node\n",
    "        if source_id not in node_attributes:\n",
    "            node_attributes[source_id] = {\n",
    "                'type': category,\n",
    "                'source_name': rel.get('source_name', 'Unknown'),\n",
    "                'types': rel.get('types', 'Unknown'),\n",
    "                'total_donation': donor_amounts.get(source_id, 0)\n",
    "            }\n",
    "            \n",
    "        # Add node if not already in graph\n",
    "        if not G.has_node(source_id):\n",
    "            G.add_node(source_id, **node_attributes[source_id])\n",
    "            \n",
    "        # Add edge\n",
    "        if G.has_node(source_id) and G.has_node(target_id):\n",
    "            G.add_edge(source_id, target_id)\n",
    "\n",
    "    print(f\"Built graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # - Optimize by using largest connected component ---\n",
    "    if G.number_of_nodes() > 300:\n",
    "        print(\"Extracting largest connected component...\")\n",
    "        components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "        if components:\n",
    "            largest_component = components[0]\n",
    "            G = G.subgraph(largest_component).copy()\n",
    "            print(f\"Reduced to largest component: {G.number_of_nodes()} nodes\")\n",
    "\n",
    "    # -Prepare Visualization Elements ---\n",
    "    # Create consistent color mapping for entity types\n",
    "    all_types = set()\n",
    "    for node in G.nodes():\n",
    "        if 'types' in G.nodes[node]:\n",
    "            types = G.nodes[node]['types']\n",
    "            if isinstance(types, list):\n",
    "                all_types.update(types)\n",
    "            else:\n",
    "                all_types.add(types)\n",
    "                \n",
    "    # Create color map for types\n",
    "    type_colors = {}\n",
    "    if all_types:\n",
    "        color_palette = plt.cm.tab20.colors\n",
    "        for i, t in enumerate(sorted(all_types)):\n",
    "            type_colors[t] = color_palette[i % len(color_palette)]\n",
    "    type_colors['candidate'] = 'gray'  # Placeholder for candidates\n",
    "\n",
    "    # Prepare node sizes and colors\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    labels = {}\n",
    "    donor_amounts_list = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        node_data = G.nodes[node]\n",
    "        \n",
    "        # Candidates: fixed size and party-based color\n",
    "        if node_data.get('type') == 'candidate':\n",
    "            # Increased candidate size\n",
    "            node_sizes.append(8000)\n",
    "            party = node_data.get('party', '')\n",
    "            node_colors.append('blue' if party == 'Democratic' else 'red')\n",
    "            labels[node] = node_data.get('name', '')\n",
    "        # Non-candidates: size based on donations, color based on type\n",
    "        else:\n",
    "            # Calculate node size based on donations\n",
    "            donation = node_data.get('total_donation', 0)\n",
    "            if donation > 0:\n",
    "                donor_amounts_list.append(donation)\n",
    "            # Increased base sizes\n",
    "            base_size = 500 if node_data.get('type') == 'endorser' else 300\n",
    "            node_sizes.append(base_size)\n",
    "            \n",
    "            # Determine color based on primary type\n",
    "            types = node_data.get('types', 'Unknown')\n",
    "            \n",
    "            # Handle different type formats\n",
    "            if isinstance(types, list):\n",
    "                # Use second type if first is \"Person\" and multiple types exist\n",
    "                if len(types) > 1 and types[0] == \"Person\":\n",
    "                    primary_type = types[1]\n",
    "                elif types:\n",
    "                    primary_type = types[0]\n",
    "                else:\n",
    "                    primary_type = 'Unknown'\n",
    "            else:\n",
    "                # Handle string types (convert to list if possible)\n",
    "                if types and \",\" in types:\n",
    "                    type_list = [t.strip() for t in types.split(\",\")]\n",
    "                    if len(type_list) > 1 and type_list[0] == \"Person\":\n",
    "                        primary_type = type_list[1]\n",
    "                    else:\n",
    "                        primary_type = type_list[0] if type_list else 'Unknown'\n",
    "                else:\n",
    "                    primary_type = types if types else 'Unknown'\n",
    "            \n",
    "            node_colors.append(type_colors.get(primary_type, 'gray'))\n",
    "            \n",
    "            # Always show source names for donors and endorsers\n",
    "            source_name = node_data.get('source_name', 'Unknown')\n",
    "            if source_name and source_name != 'Unknown':\n",
    "                labels[node] = source_name[:20]  # Limit name length\n",
    "            else:\n",
    "                # Fallback to node ID if no name\n",
    "                labels[node] = f\"{node_data.get('type', 'Entity')} {str(node)[:6]}\"\n",
    "\n",
    "    # Scale donor sizes based on actual amounts\n",
    "    if donor_amounts_list:\n",
    "        min_don = min(donor_amounts_list)\n",
    "        max_don = max(donor_amounts_list)\n",
    "        log_min = np.log(min_don) if min_don > 0 else 0\n",
    "        log_max = np.log(max_don) if max_don > 0 else 1\n",
    "        \n",
    "        for i, node in enumerate(G.nodes()):\n",
    "            node_data = G.nodes[node]\n",
    "            if node_data.get('type') == 'donor':\n",
    "                donation = node_data.get('total_donation', 0)\n",
    "                if donation > 0:\n",
    "                    log_don = np.log(donation)\n",
    "                    # Increased size range for donors\n",
    "                    scaled_size = 300 + 1500 * (log_don - log_min) / (log_max - log_min)\n",
    "                    node_sizes[i] = scaled_size\n",
    "\n",
    "    # - Visualize the Graph ---\n",
    "    try:\n",
    "        # Larger figure size\n",
    "        plt.figure(figsize=(24, 18))\n",
    "        print(\"Calculating graph layout...\")\n",
    "        \n",
    "        # Choose layout based on graph size\n",
    "        if G.number_of_nodes() > 100:\n",
    "            # Increased k value for better spacing\n",
    "            pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
    "        else:\n",
    "            pos = nx.kamada_kawai_layout(G)\n",
    "        \n",
    "        print(\"Drawing graph...\")\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.7)\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, \n",
    "            node_color=node_colors, \n",
    "            node_size=node_sizes, \n",
    "            alpha=0.8\n",
    "        )\n",
    "        nx.draw_networkx_labels(\n",
    "            G, pos, \n",
    "            labels=labels, \n",
    "            # Decreased font size\n",
    "            font_size=8, \n",
    "            font_color='black',\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.1')\n",
    "        )\n",
    "        \n",
    "        # Create custom legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', \n",
    "                      markersize=10, label='Democratic Candidate'),\n",
    "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', \n",
    "                      markersize=10, label='Republican Candidate')\n",
    "        ]\n",
    "        \n",
    "        # Add legend items for entity types\n",
    "        for t, color in type_colors.items():\n",
    "            if t != 'candidate':  # Skip placeholder\n",
    "                legend_elements.append(\n",
    "                    plt.Line2D([0], [0], marker='o', color='w', \n",
    "                              markerfacecolor=color, markersize=8, label=t)\n",
    "                )\n",
    "                \n",
    "        plt.legend(\n",
    "            handles=legend_elements, \n",
    "            loc='upper right', \n",
    "            title=\"Entity Types\",\n",
    "            fontsize=8,\n",
    "            title_fontsize=10\n",
    "        )\n",
    "        \n",
    "        plt.title(\n",
    "            f\"Enhanced Candidate Network for {year_to_plot}\\n\"\n",
    "            f\"(Donors > ${min_donation}, Size = Donation Amount, Color = Entity Type)\",\n",
    "            size=16\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        \n",
    "        output_filename = f\"enhanced_network_{year_to_plot}.png\"\n",
    "        plt.savefig(output_filename, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Successfully saved enhanced visualization to {output_filename}\")\n",
    "        print(f\"Node colors represent entity types, size represents donation amount\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during enhanced visualization: {e}\")\n",
    "        # Fallback to basic visualization\n",
    "        print(\"Attempting basic visualization...\")\n",
    "        basic_visualization(G, candidate_info_this_year, year_to_plot, min_donation)\n",
    "\n",
    "\n",
    "def basic_visualization(G, candidate_info, year, min_donation):\n",
    "    \"\"\"Fallback visualization without enhanced features\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "        \n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        labels = {}\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            if G.nodes[node].get('type') == 'candidate':\n",
    "                party = G.nodes[node].get('party', '')\n",
    "                node_colors.append('blue' if party == 'Democratic' else 'red')\n",
    "                # Increased size in basic visualization too\n",
    "                node_sizes.append(5000)\n",
    "                labels[node] = G.nodes[node].get('name', '')\n",
    "            else:\n",
    "                node_colors.append('lightgray')\n",
    "                # Increased size in basic visualization too\n",
    "                node_sizes.append(500)\n",
    "                # Show names in basic visualization too\n",
    "                source_name = G.nodes[node].get('source_name', 'Unknown')\n",
    "                if source_name and source_name != 'Unknown':\n",
    "                    labels[node] = source_name[:20]\n",
    "                else:\n",
    "                    labels[node] = f\"{G.nodes[node].get('type', 'Entity')} {str(node)[:6]}\"\n",
    "                \n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)\n",
    "        # Smaller font in basic visualization\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        plt.title(f\"Basic Network for {year} (Donors > ${min_donation})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(f\"basic_network_{year}.png\", dpi=100)\n",
    "        plt.close()\n",
    "        print(\"Saved basic visualization as fallback\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create basic visualization: {e}\")\n",
    "\n",
    "def plot_donations_by_type(year_to_plot='2020', min_donation=1000):\n",
    "    \"\"\"\n",
    "    Creates clearly separated visualizations showing donations by type for each candidate\n",
    "    with distinct formatting and spacing, excluding candidates' own donations\n",
    "    Uses logarithmic scaling for donation amounts\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    relationships = load_data('relationships.json')\n",
    "    candidates = load_data('candidates.json')\n",
    "\n",
    "    if not relationships or not candidates:\n",
    "        print(\"Could not load necessary data files.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Identify candidates and their IDs ---\n",
    "    candidate_info = {}\n",
    "    candidate_ids = set()\n",
    "    candidate_names = {}\n",
    "    \n",
    "    # Identify candidates for the year\n",
    "    for key, info in candidates.items():\n",
    "        year, party = key\n",
    "        if year == year_to_plot and info.get('id'):\n",
    "            cand_id = info['id']\n",
    "            candidate_info[cand_id] = {\n",
    "                'name': info['name'], \n",
    "                'party': party\n",
    "            }\n",
    "            candidate_ids.add(cand_id)\n",
    "            candidate_names[info['name']] = cand_id  # Map name to ID\n",
    "    \n",
    "    if not candidate_info:\n",
    "        print(f\"No candidate data found for {year_to_plot}.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Aggregate donations by candidate and type ---\n",
    "    candidate_donations = {cand_id: defaultdict(float) for cand_id in candidate_info}\n",
    "    \n",
    "    # Create a set of candidate names to exclude self-donations\n",
    "    candidate_names_set = set(candidate_info[cid]['name'] for cid in candidate_ids)\n",
    "    \n",
    "    for key, rel_list in relationships.items():\n",
    "        year, party = key\n",
    "        if year == year_to_plot:\n",
    "            for rel in rel_list:\n",
    "                if rel.get('category') != 'donor':\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    amount = float(rel.get('amount', 0))\n",
    "                    if amount < min_donation:\n",
    "                        continue\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                \n",
    "                source_id = rel.get('source')\n",
    "                target_id = rel.get('target')\n",
    "                types = rel.get('types', [])\n",
    "                source_name = rel.get('source_name', '')\n",
    "                \n",
    "                # Skip if source is a candidate (by name or ID)\n",
    "                if (source_id in candidate_ids) or (source_name in candidate_names_set):\n",
    "                    continue\n",
    "                \n",
    "                # Skip if target not a candidate or types missing\n",
    "                if not types or target_id not in candidate_info:\n",
    "                    continue\n",
    "                \n",
    "                # Determine primary type (use second if first is \"Person\")\n",
    "                if isinstance(types, list):\n",
    "                    if len(types) > 1 and types[0] == \"Person\":\n",
    "                        primary_type = types[1]\n",
    "                    elif types:\n",
    "                        primary_type = types[0]\n",
    "                    else:\n",
    "                        primary_type = 'Unknown'\n",
    "                else:\n",
    "                    if \",\" in types:\n",
    "                        type_list = [t.strip() for t in types.split(\",\")]\n",
    "                        if len(type_list) > 1 and type_list[0] == \"Person\":\n",
    "                            primary_type = type_list[1]\n",
    "                        else:\n",
    "                            primary_type = type_list[0] if type_list else 'Unknown'\n",
    "                    else:\n",
    "                        primary_type = types if types else 'Unknown'\n",
    "                \n",
    "                # Aggregate donations\n",
    "                candidate_donations[target_id][primary_type] += amount\n",
    "\n",
    "    # --- 3. Prepare data for visualization ---\n",
    "    dem_data = []\n",
    "    rep_data = []\n",
    "    dem_candidate = None\n",
    "    rep_candidate = None\n",
    "    \n",
    "    # Get candidate-specific data\n",
    "    for cand_id, donations in candidate_donations.items():\n",
    "        if candidate_info[cand_id]['party'] == 'Democratic':\n",
    "            dem_candidate = candidate_info[cand_id]['name']\n",
    "            dem_data = sorted(donations.items(), key=lambda x: x[1], reverse=True)\n",
    "        elif candidate_info[cand_id]['party'] == 'Republican':\n",
    "            rep_candidate = candidate_info[cand_id]['name']\n",
    "            rep_data = sorted(donations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if not dem_data and not rep_data:\n",
    "        print(f\"No donation data found for {year_to_plot} with donations > ${min_donation}.\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Create visualizations with logarithmic scaling ---\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    fig.suptitle(f'External Donations by Type in {year_to_plot} Election\\n(Log Scale, Minimum donation: ${min_donation}, Excluding Self-Donations)', \n",
    "                fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Create separate subplots with more space between them\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[1, 1], wspace=0.4)\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    \n",
    "    # Shared color mapping for both charts\n",
    "    all_types = set([t for t, _ in dem_data] + [t for t, _ in rep_data])\n",
    "    color_palette = plt.cm.tab20.colors\n",
    "    type_colors = {t: color_palette[i % len(color_palette)] for i, t in enumerate(sorted(all_types))}\n",
    "    \n",
    "    # Function to create a candidate's chart with distinct styling and log scaling\n",
    "    def create_log_candidate_chart(ax, data, candidate_name, party, color):\n",
    "        if not data:\n",
    "            ax.text(0.5, 0.5, \"No donation data\", \n",
    "                    ha='center', va='center', fontsize=14)\n",
    "            ax.set_title(f\"{party} Candidate: {candidate_name}\", fontsize=14)\n",
    "            return\n",
    "            \n",
    "        types = [t[0] for t in data]\n",
    "        amounts = [t[1] for t in data]\n",
    "        \n",
    "        # Create bars with consistent colors\n",
    "        colors = [type_colors[t] for t in types]\n",
    "        bars = ax.bar(types, amounts, color=colors, edgecolor='black', linewidth=0.7)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height * 1.01,\n",
    "                    f\"${height/1e6:,.1f}M\" if height > 1e6 else f\"${height/1e3:,.0f}K\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Set logarithmic scale for y-axis\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # Format y-axis with log scale labels\n",
    "        ax.yaxis.set_major_formatter(\n",
    "            plt.FuncFormatter(lambda x, loc: \"${:,.0f}M\".format(x/1e6) if x >= 1e6 else \n",
    "                             \"${:,.0f}K\".format(x/1e3) if x >= 1e3 else f\"${x:,.0f}\")\n",
    "        )\n",
    "        \n",
    "        # Add grid lines for log scale\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "        \n",
    "        # Set appropriate y-axis limits\n",
    "        min_val = min(amounts) * 0.8 if min(amounts) > 0 else 1\n",
    "        max_val = max(amounts) * 1.2\n",
    "        ax.set_ylim(min_val, max_val)\n",
    "        \n",
    "        # Add distinct background color\n",
    "        ax.set_facecolor((*plt.cm.colors.to_rgb(color), 0.1))\n",
    "        \n",
    "        # Add border around each subplot\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(color)\n",
    "            spine.set_linewidth(2)\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f\"{party} Candidate: {candidate_name}\", fontsize=14, color=color, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # Set distinct border\n",
    "        ax.spines['top'].set_visible(True)\n",
    "        ax.spines['right'].set_visible(True)\n",
    "        ax.spines['bottom'].set_visible(True)\n",
    "        ax.spines['left'].set_visible(True)\n",
    "    \n",
    "    # Create charts for each candidate with distinct styling and log scaling\n",
    "    create_log_candidate_chart(ax1, dem_data, dem_candidate, 'Democratic', 'blue')\n",
    "    create_log_candidate_chart(ax2, rep_data, rep_candidate, 'Republican', 'red')\n",
    "    \n",
    "    # Add legend at bottom\n",
    "    if len(all_types) <= 20:\n",
    "        legend_handles = [plt.Rectangle((0,0),1,1, color=type_colors[t]) for t in sorted(all_types)]\n",
    "        fig.legend(legend_handles, sorted(all_types), \n",
    "                  loc='lower center', ncol=min(5, len(all_types)), \n",
    "                  fontsize=10, title='Donor Types', title_fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05 if len(all_types) <= 20 else 0, 1, 0.95])\n",
    "    \n",
    "    output_filename = f\"log_donations_by_type_{year_to_plot}.png\"\n",
    "    plt.savefig(output_filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Successfully saved log-scaled donations visualization to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cf31f",
   "metadata": {},
   "source": [
    "Main workflow"
   ]
  },
   "cell_type": "code",
   "execution_count": 144,
   "id": "5c0a73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    candidates = load_data('candidates.json')\n",
    "    relationships = load_data('relationships.json')\n",
    "    if not candidates or not relationships:\n",
    "        print(\"Data files not found or empty. Please check collection function and file path.\")\n",
    "        return\n",
    "    election_years = range(1988, 2025, 4)\n",
    "    for year in election_years:\n",
    "    #    plot_donor_network(year_to_plot=str(year), min_donation=10000, label_threshold=100000)\n",
    "    #    plot_donations_by_type(year_to_plot=str(year), min_donation=5000)\n",
    "        mind, mint, thr = 0,0,0\n",
    "        if year < 2000:\n",
    "            mind,mint,thr = 2500,1000,10000\n",
    "        elif year < 2008:\n",
    "            mind,mint,thr = 10000,5000,100000\n",
    "        elif year < 2016:\n",
    "            mind,mint,thr = 80000,40000,1000000\n",
    "        elif year > 2016:\n",
    "            mind,mint,thr = 10000,5000,100000\n",
    "        else:\n",
    "            mind,mint,thr = 15000,5000,100000\n",
    "        plot_donor_network(year_to_plot=str(year), min_donation=mind, label_threshold=thr)\n",
    "        plot_donations_by_type(year_to_plot=str(year), min_donation=mint)\n",
    "    \n",
    "    election_years = sorted(set(int(y) for y, _ in candidates.keys()))\n",
    "    graphs = build_election_graphs(candidates, relationships)\n",
    "    graph_2024 = graphs.pop(2024, None)\n",
    "    resilience_evaluator = ResilienceEvaluator()\n",
    "    \n",
    "    k_folds = 4\n",
    "    if len(election_years) < k_folds:\n",
    "        print(f\"Warning: Not enough election years ({len(election_years)}) for {k_folds}-fold cross-validation.\")\n",
    "        k_folds = len(election_years)\n",
    "\n",
    "    # Store results for comparison\n",
    "    results = {\n",
    "        'with_optimization': {'win_acc': [], 'vote_acc': [], 'resilience': []},\n",
    "        'without_optimization': {'win_acc': [], 'vote_acc': [], 'resilience': []}\n",
    "    }\n",
    "\n",
    "    # Run comparison for both versions\n",
    "    for use_optimization in [True, False]:\n",
    "        version = 'with_optimization' if use_optimization else 'without_optimization'\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Running {version.replace('_', ' ').title()} Version\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        genetic_optimizer = GeneticRewiringOptimizer(\n",
    "            model=None,\n",
    "            fitness_evaluator=resilience_evaluator,\n",
    "            population_size=8,\n",
    "            generations=3\n",
    "        )\n",
    "        \n",
    "        fold_size = len(election_years) // k_folds\n",
    "        val_accuracies_win = []\n",
    "        val_accuracies_vote = []\n",
    "        \n",
    "        for fold in range(k_folds):\n",
    "            print(f\"\\nFold {fold + 1}/{k_folds} ({version})\")\n",
    "            val_start = fold * fold_size\n",
    "            val_end = (fold + 1) * fold_size if fold < k_folds - 1 else len(election_years)\n",
    "            val_years = election_years[val_start:val_end]\n",
    "            train_years = [y for y in election_years if y not in val_years and y != 2024]\n",
    "            test_years = [y for y in graphs.keys() if y in val_years]\n",
    "            train_set = {y: deepcopy(graphs[y]) for y in train_years}  # Deepcopy to preserve original\n",
    "            test_set = {y: deepcopy(graphs[y]) for y in test_years}\n",
    "            \n",
    "            if not train_set or not test_set:\n",
    "                print(\"Train or Test set is empty. Cannot proceed.\")\n",
    "                continue\n",
    "                \n",
    "            in_channels = train_set[list(train_set.keys())[0]].num_node_features\n",
    "            model = GNN(in_channels=in_channels, hidden_channels=64, heads=4)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "            \n",
    "            if use_optimization:\n",
    "                genetic_optimizer.model = model\n",
    "\n",
    "            print(f\"\\nTraining GAT model ({version})...\")\n",
    "            for epoch in range(101):\n",
    "                loss = train_loop(model, train_set, optimizer, adversarial_strength=0.25)\n",
    "                if epoch % 25 == 0:\n",
    "                    train_acc, train_vote, train_mae = test_loop(model, train_set)\n",
    "                    test_acc, test_vote, test_mae = test_loop(model, test_set)\n",
    "                    print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train Acc(win): {train_acc:.4f}, Train Acc(vote): {train_vote:.4f}') \n",
    "                    print(f'{\" \"*8}Test MAE: {test_mae:.2f}, Test Acc(win): {test_acc:.4f}, Test Acc(vote): {test_vote:.4f}')\n",
    "                    \n",
    "                # Apply optimization only if enabled and at the end\n",
    "                if use_optimization and epoch == 100:\n",
    "                    for year in [2020, 2016]:\n",
    "                        if year in train_set:\n",
    "                            data = train_set[year]\n",
    "                            optimized_graph = genetic_optimizer.optimize(data, data.cand_A_idx)\n",
    "                            train_set[year] = optimized_graph\n",
    "            \n",
    "            # Store test results\n",
    "            win_acc, vote_acc, vote_mae = test_loop(model, test_set)\n",
    "            results[version]['win_acc'].append(win_acc)\n",
    "            results[version]['vote_acc'].append(vote_acc)\n",
    "            \n",
    "            # Calculate and store resilience scores\n",
    "            fold_resilience = []\n",
    "            for year in test_years:\n",
    "                data = test_set[year]\n",
    "                dem_res = resilience_evaluator.evaluate(model, data, data.cand_A_idx)\n",
    "                rep_res = resilience_evaluator.evaluate(model, data, data.cand_B_idx)\n",
    "                fold_resilience.append((dem_res + rep_res) / 2)  # Average resilience\n",
    "            results[version]['resilience'].append(np.mean(fold_resilience))\n",
    "            \n",
    "            # Print fold results\n",
    "            print(f\"\\nFold {fold+1} Results ({version}):\")\n",
    "            print(f\"  Win Accuracy: {win_acc:.4f}\")\n",
    "            print(f\"  Vote Accuracy: {vote_acc:.4f}\")\n",
    "            print(f\"  Resilience: {np.mean(fold_resilience):.4f}\")\n",
    "    \n",
    "    # Print overall results\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Final Cross-Validation Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for version in ['with_optimization', 'without_optimization']:\n",
    "        win_accs = results[version]['win_acc']\n",
    "        vote_accs = results[version]['vote_acc']\n",
    "        resilience = results[version]['resilience']\n",
    "        \n",
    "        print(f\"\\n{version.replace('_', ' ').title()} Version:\")\n",
    "        print(f\"  Win Accuracy: {np.mean(win_accs):.4f} ± {np.std(win_accs):.4f}\")\n",
    "        print(f\"  Vote Accuracy: {np.mean(vote_accs):.4f} ± {np.std(vote_accs):.4f}\")\n",
    "        print(f\"  Resilience: {np.mean(resilience):.4f} ± {np.std(resilience):.4f}\")\n",
    "    \n",
    "    # Train final models for robustness testing and prediction\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Training Final Models for Robustness Testing\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Prepare training and test sets\n",
    "    train_years = [y for y in election_years if y not in [2016, 2020] and y != 2024]\n",
    "    test_years = [2016, 2020]\n",
    "    train_set = {y: deepcopy(graphs[y]) for y in train_years}\n",
    "    test_set = {y: deepcopy(graphs[y]) for y in test_years}\n",
    "    \n",
    "    # Train optimized model\n",
    "    print(\"\\nTraining optimized model...\")\n",
    "    in_channels = train_set[list(train_set.keys())[0]].num_node_features\n",
    "    model_opt = GNN(in_channels=in_channels, hidden_channels=64, heads=4)\n",
    "    optimizer_opt = optim.Adam(model_opt.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    genetic_optimizer_opt = GeneticRewiringOptimizer(\n",
    "        model=model_opt,\n",
    "        fitness_evaluator=resilience_evaluator,\n",
    "        population_size=8,\n",
    "        generations=3\n",
    "    )\n",
    "    \n",
    "    for epoch in range(101):\n",
    "        loss = train_loop(model_opt, train_set, optimizer_opt, adversarial_strength=0.25)\n",
    "        if epoch % 25 == 0:\n",
    "            train_acc, train_vote, train_mae = test_loop(model_opt, train_set)\n",
    "            test_acc, test_vote, test_mae = test_loop(model_opt, test_set)\n",
    "            print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train Acc(win): {train_acc:.4f}, Train Acc(vote): {train_vote:.4f}') \n",
    "            print(f'{\" \"*8}Test MAE: {test_mae:.2f}, Test Acc(win): {test_acc:.4f}, Test Acc(vote): {test_vote:.4f}')\n",
    "        if epoch == 100:  # Apply optimization at the end\n",
    "            for year in [2020, 2016]:\n",
    "                if year in train_set:\n",
    "                    data = train_set[year]\n",
    "                    optimized_graph = genetic_optimizer_opt.optimize(data, data.cand_A_idx)\n",
    "                    train_set[year] = optimized_graph\n",
    "    \n",
    "    # Train standard model\n",
    "    print(\"\\nTraining standard model...\")\n",
    "    model_std = GNN(in_channels=in_channels, hidden_channels=64, heads=4)\n",
    "    optimizer_std = optim.Adam(model_std.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(101):\n",
    "        loss = train_loop(model_std, train_set, optimizer_std, adversarial_strength=0.25)\n",
    "        if epoch % 25 == 0:\n",
    "            train_acc, train_vote, train_mae = test_loop(model_std, train_set)\n",
    "            test_acc, test_vote, test_mae = test_loop(model_std, test_set)\n",
    "            print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train Acc(win): {train_acc:.4f}, Train Acc(vote): {train_vote:.4f}') \n",
    "            print(f'{\" \"*8}Test MAE: {test_mae:.2f}, Test Acc(win): {test_acc:.4f}, Test Acc(vote): {test_vote:.4f}')\n",
    "    \n",
    "    # Robustness testing\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Testing Robustness to Missing Edges\") \n",
    "    removal_rates = [0.02, 0.05, 0.1, 0.2, 0.5]\n",
    "    corrupted_graphs = create_missing_edge_graphs(test_set, removal_rates)\n",
    "    \n",
    "    print(\"\\nEvaluating Optimized Model:\")\n",
    "    opti_win_accs, opti_vote_accs, opti_vote_maes = evaluate_corruption_robustness(\n",
    "        model_opt, corrupted_graphs, removal_rates\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating Standard Model:\")\n",
    "    std_win_accs, std_vote_accs, std_vote_maes = evaluate_corruption_robustness(\n",
    "        model_std, corrupted_graphs, removal_rates\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAnalyzing overall influence across all elections...\")\n",
    "    all_graphs = {**graphs}\n",
    "    if graph_2024:\n",
    "        all_graphs[2024] = graph_2024\n",
    "    \n",
    "    relationship_categories = {0: \"donor\", 1: \"endorser\", 2: \"ownership\"}\n",
    "    top_influential, full_metrics = analyze_influence_across_years(\n",
    "        model_opt, \n",
    "        all_graphs,\n",
    "        relationship_categories\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTop 5 Influential Entity Types (Averaged Across Elections):\")\n",
    "    for i, (entity_type, score) in enumerate(top_influential):\n",
    "        metrics = full_metrics[entity_type]\n",
    "        print(f\"{i+1}. {entity_type}: {score:.4f}\")\n",
    "        print(f\"   Avg Attention: {metrics['attention']:.6f}\")\n",
    "        print(f\"   Avg SHAP: {metrics['shap']:.6f}\")\n",
    "        print(f\"   Avg F1-Drop: {metrics['f1_drop']:.6f}\")\n",
    "                \n",
    "    # 2024 Prediction\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"2024 Election Prediction\")\n",
    "    if graph_2024:\n",
    "        model_opt.eval()\n",
    "        with torch.no_grad():\n",
    "            win_logits, vote_pred, _ = model_opt(graph_2024)\n",
    "        \n",
    "        win_probs = F.softmax(win_logits, dim=1).squeeze()\n",
    "        prob_A, prob_B = win_probs[1].item(), win_probs[0].item()\n",
    "        \n",
    "        vote_A_norm, vote_B_norm = vote_pred[1].item(), vote_pred[0].item()\n",
    "        total_norm = vote_A_norm + vote_B_norm\n",
    "        vote_A = round(vote_A_norm / total_norm * 538) if total_norm > 0 else 269\n",
    "        vote_B = 538 - vote_A\n",
    "\n",
    "        print(f\"\\nCandidate ({graph_2024.cand_A_name} - Dem): Win = {prob_A:.2%}, Votes = {vote_A}\")\n",
    "        print(f\"Candidate ({graph_2024.cand_B_name} - Rep): Win = {prob_B:.2%}, Votes = {vote_B}\")\n",
    "        \n",
    "        # Resilience analysis\n",
    "        dem_resilience = resilience_evaluator.evaluate(model_opt, graph_2024, graph_2024.cand_A_idx)\n",
    "        rep_resilience = resilience_evaluator.evaluate(model_opt, graph_2024, graph_2024.cand_B_idx)\n",
    "        avg_resilience = (dem_resilience + rep_resilience) / 2\n",
    "        print(f\"\\nAverage Resilience Score: {avg_resilience:.2%}\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b356fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
